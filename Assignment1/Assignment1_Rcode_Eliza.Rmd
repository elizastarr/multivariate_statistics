---
title: "Assignment 1"
output: html_notebook
---

```{r}
df <- read.csv(file="Assignment1_Data.csv")
```

### Question 1
For each of the 17 variables in the data file, indicate of what type they are and
explain why. Choose from nominal scale, ordinal scale, interval scale or ratio scale.
```{r}
data_types <- function(frame) {
  res <- lapply(frame, class)
  res_frame <- data.frame(unlist(res))
  barplot(table(res_frame), main="Data Types", col="steelblue", ylab="Number of Features")
}
data_types(df)
```
### Question 2
What is the percentage of companies that have introduced new or significantly
improved products/goods?
```{r}
ProdInn1 <- df[(df$ProductInnovation==1),]
ProdInn0 <- df[(df$ProductInnovation==0),]

perc <- dim(ProdInn1)[1]/dim(df)[1]*100
cat(round(perc,2), "%")
```

### Question 3
Compare the distribution of R&D expenditures for companies that have introduced
new or significantly improved products/goods, versus companies that have not.
Use a plot and a statistical test. Explain which plot and test you use, and why.
What do you conclude?
```{r}
# heavily skewed to the right
summary(ProdInn1$RDExp)
summary(ProdInn0$RDExp)
```

```{r}
# Take the log of the data so the distribution can be visualized better

hist(log(ProdInn1$RDExp), breaks=100, col='blue')
hist(log(ProdInn0$RDExp), breaks=100, col='red')
```
```{r}
# The RDExp of companies who did innovate has a different distribution from that of those who did not innovate
ks.test(log(ProdInn1$RDExp), log(ProdInn0$RDExp))
```


### Question 4
Is manufacturing innovation (i.e., new or significantly improved manufacturing
process and/or procedures for service delivery) more common for companies that
have introduced new or significantly improved products/goods, versus companies
that have not? Support your answer.

```{r}
# Manufacturing innovation is not more common for companies that innovated their products/goods, versus companies that did not.
cat(dim(ProdInn1[ProdInn1$ManufacturingInnovation==1,])[1]/dim(ProdInn1)[1]*100,"% product=1, manuf=1\n")
cat(dim(ProdInn1[ProdInn1$ManufacturingInnovation==0,])[1]/dim(ProdInn1)[1]*100,"% product=1, manuf=0\n")

cat(dim(ProdInn0[ProdInn1$ManufacturingInnovation==1,])[1]/dim(ProdInn0)[1]*100,"% product=0, manuf=1\n")
cat(dim(ProdInn0[ProdInn1$ManufacturingInnovation==0,])[1]/dim(ProdInn0)[1]*100,"% product=0, manuf=0\n")

#chisq.test()
```

### Question 5
Which variable(s) contain missing values?
```{r}
# NrEmployees has missing values
colSums(is.na(df))
```

### Question 6
What is the percentage of missingness?
```{r}
cat(mean(is.na(df))*100,"% of all data points are missing.\n")
cat(mean(is.na(df$NrEmployees))*100, "% of the NrEmployees column is missing.")
```

### Question 7
Is the missingness related to any of the other variables in the data set? Explain
which tests you use and why.
```{r}
# Missingness is not related to any of the other variables in the dataset.
# Used 2-sample t-test to compare the mean of each variable of complete cases with that of incomplete cases

missing <- df[!complete.cases(df),]
complete <- df[complete.cases(df),]

for (i in colnames(df)){
  if(i!="NrEmployees"){
    print(i)
    print(t.test(missing[[i]],complete[[i]])$p.value)
  }
}
```

### Question 8
What do you conclude about the type of missingness from your tests of the previous
question?

I conclude that the data is missing at random and can be omitted from the dataset.

### Question 9
Compute the Mahalanobis distances of the companies based on the information
variables. Graphically present the Mahalanobis distances.

```{r}
complete
```

```{r}
maha_distance <- complete[,c(8:17)]
dist <- mahalanobis(complete, center=colMeans(complete), cov=cov(complete))
maha_distance$dist <- round(dist, 2)

maha_distance$outlier_maha <- 0
maha_distance$outlier_maha[maha_distance$dist > 3] <- 1

upper.whisker <- boxplot(maha_distance$dist)$stats[5]
```

### Question 10
Can we use the rule-of-thumb that observations of which the Mahalanobis Distance
divided q is larger 3 are outliers? Here, q is the number of variables used in the
calculation of the Mahalanobis distance. Explain why or why not.

```{r}
# Do not use 3 as the rule of thumb because the data is not normally distributed.
mean(maha_distance$outlier_maha)
```


### Question 11
Which companies, if any, are outliers? Describe the decision process by which you
have or have not identified outliers. If you have identified outliers, perform the
rest of the assignment after dropping the outliers from the analysis.

```{r}
# only 6 outliers when using the upper whisker as the threshold (54.57)
outliers <- complete[maha_distance$dist>upper.whisker,]
no_outliers <- complete[maha_distance$dist<=upper.whisker,]
outliers
```


### Question 12
Is a factor analysis appropriate for the questions on the information sources of
innovations? Explain how you have checked whether factor analysis is appropriate.

Yes, factor analysis is appropriate because there exists a positive correlation (greater than .5 even) between several of the variables.

```{r}
library(psych)
library(ggplot2)
# make heatmap
cormat <- round(cor(no_outliers[,c(8:17)]),2)
cormat

melted_cormat <- reshape2::melt(cormat)

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
```


### Question 13
Make a scree plot. How many factors does the scree plot suggest?
```{r}
# 2 factors
EigenDecomp <- eigen(cov(no_outliers[,c(8:17)]))
EigenValues <- EigenDecomp$values
plot(EigenValues,main="Scree Plot IAS",type="l", xlab="Number of Factors",ylab="Eigen Values")
```


### Question 14
For what purpose is a rotation of the factors in an exploratory factor analysis used?
Should we perform a rotation in this setting? Why or why not?

Rotate so that the factor loadings are easier to interpret. For example, consider 2 factor loadings F1 and F2 with variables X1 and X2 in a cluster with higher loadings for F1 and X3 and X4 with higher loadings for F2. When we perform a varimax rotation, we will create new factor loadings that give variables X1 and X2 high values for F1 and low for F2, and vice versa for variables X3 and X4.

### Question 15
Perform factor analysis using the number of factors as suggested by the scree plot.
Clearly state how many factors you have included and whether or not you have
used a rotation. Report the estimated uniquenesses and factor loadings.

I am including all 10 information variables and am using rotation. The algroithm has converged. Two factors are not sufficent because Chi-squared null hypotheses is rejected.
```{r}
k<-2
EFA.kfactors <- factanal(no_outliers[,c(8:17)], factors=k, rotation="varimax")
EFA.kfactors$converged
EFA.kfactors
```


### Question 16
Check whether all the variables should be retained in the factor analysis.
(a) Which criteria do you use? Which of the variables, if any, should be dropped?

We will first use uniqueness as the critera for dropping variables. If we use 0.8 as the uniquness threshold, no variables are dropped. However, Info5 has a distinctly high uniquness of 0.743. To further investigate factor 5, we will look at its factor loadings when 2, 3, 4, and 5 factors are used. If Info 5 is also cross-loading or has low loadings on all factors, this could further support the decision to remove Info5.
```{r}
EFA.kfactors$uniquenesses[EFA.kfactors$uniquenesses>.8]
```

Let us look at the factor loadings of k=[2,6], since the Chi-squared test indicates that 2 factors are not sufficient. Indeed, Info5 does not score high on any one factor for k=[2,5]. We should remove factor 5 from the analysis.
```{r}
# Factor 4, 5, and 6 don't have high communality
# Info 4 does not score very high on one factor
print("k=5")
EFA.kfactors <- factanal(no_outliers[,c(8:17)], factors=6, rotation="varimax")
EFA.kfactors$loadings

# Factor 3,4, and 5 don't have high communality
# Info 5, 9 do not score very high on one factor
print("k=5")
EFA.kfactors <- factanal(no_outliers[,c(8:17)], factors=5, rotation="varimax")
EFA.kfactors$loadings

# Factor 4 doesn't have high communality
# Info 5 do not score high on one factor
# Explained variance is getting low
print("k=4")
EFA.kfactors <- factanal(no_outliers[,c(8:17)], factors=4, rotation="varimax")
EFA.kfactors$loadings

# All factors have high communality
# Info 5 and 8 do not score high on one factor
# Explained variance is low
print("k=3")
EFA.kfactors <- factanal(no_outliers[,c(8:17)], factors=3, rotation="varimax")
EFA.kfactors$loadings

# All factors have high communality
# Info 5 do not score high on one factor
# Explained variance is low
print("k=2")
EFA.kfactors <- factanal(no_outliers[,c(8:17)], factors=2, rotation="varimax")
EFA.kfactors$loadings
```


(b) If one or more of the variables should be dropped, redo the scree plot and the
estimation of the factor model with the number of factors suggested by the
scree plot.

```{r}
# Plot still indicates 2 Factors if we were to remove Info5.
EigenDecomp <- eigen(cov(no_outliers[,c(8:11,13:17)]))
EigenValues <- EigenDecomp$values
plot(EigenValues,main="Scree Plot IAS",type="l", xlab="Number of Factors",ylab="Eigen Values")
```

The test states that 2 factors is still not sufficent.
```{r}
EFA.kfactors <- factanal(no_outliers[,c(8:11,13:17)], factors=2, rotation="varimax")
EFA.kfactors
```


(c) If needed, make further changes to the factor model based on the output of
the factor analysis. Report and motivate all the steps you have taken to arrive
at a final factor model. How many factors are included in your final model?

I would choose 3 factors in the final model, despite the failed chi-squared test because 3 factors gives communality for all factor loadings, only variable Info8 cross-loads, and the total explained variance is higher than that of a 2 factor model, and the grouping of variables is logical.

```{r}
# Before dropping Info5, none of the chi-squared tests failed to reject the null hypothesis.
# After dropping Info5, the Chi-squared test suggests that 5 factors are sufficent, but we need to make sure that makes sense when looking at the factor loadings.
NrFactors <- 1:5
pvals <- rep(NA,times=length(NrFactors))
for(k in NrFactors){
  # Estimate k-factor model:
  Estimation <- factanal(no_outliers[,c(8:11,13:17)],factors=k,rotation="varimax")
  pvals[k] <- Estimation$PVAL
}

cbind(NrFactors,pvals)
```

Each factor should have a high loading on several variables (communality) and each variable should load high on one factor loading and variable groupings should make sense.

```{r}

# Factor 5 and 4 don't have high communality
print("k=5")
EFA.kfactors <- factanal(no_outliers[,c(8:11,13:17)], factors=5, rotation="varimax")
EFA.kfactors$loadings

# Factor 4 and 3 don't have high communality
print("k=4")
EFA.kfactors <- factanal(no_outliers[,c(8:11,13:17)], factors=4, rotation="varimax")
EFA.kfactors$loadings

# All factors have high communality
# Info 8 cross-loads
# Explained variance is low
print("k=3")
EFA.kfactors <- factanal(no_outliers[,c(8:11,13:17)], factors=3, rotation="varimax")
EFA.kfactors$loadings

# Both factors have high communality
print("k=2")
EFA.kfactors <- factanal(no_outliers[,c(8:11,13:17)], factors=2, rotation="varimax")
EFA.kfactors$loadings
```

(d) Report the set of included variables, the estimated uniquenesses and factor
loadings of your final factor model. Continue the next questions with the
model you prefer.

```{r}
EFA.kfactors <- factanal(no_outliers[,c(8:11,13:17)], factors=3, rotation="varimax")
EFA.kfactors
```


### Question 17
Which names do you suggest for the factors? Motivate the names you have chosen.
```{r}
f1 <- c(8,9,10,11)
f2 <- c(13, 14)
f3 <- c(15,16,17)
```

3 Factors
1. Info1, Info2, Info 3, Info4: Industry
  - Suppliers of facilities, equipment, components, software and services
  - Sources within your firm or enterprise group
  - Customers or principals
  - Competitor or other enterprise in your industry sector
2. Info6, Info7: Research
  - Universities, universities of applied science or other higher education institutions
  - National or non-profit research institutions
3. Info8, Info9, Info10: Other
  - Conferences, fairs, exhibitions
  - Scientific periodicals and specialty publications
  - Associations and boards

### Question 18
Perform a reliability analysis of each factor.
(a) Which criterion do you use? Which of the variables, if any, should be
dropped?

Using Cronbach’s alpha, we decide to keep all variables in all factors.
```{r}
# The closer alpha is to 1, the better
# No variables should be dropped from f1, f2, or f3

f1_alpha <- psych::alpha(no_outliers[,f1])
f2_alpha <- psych::alpha(no_outliers[,f2])
f3_alpha <- psych::alpha(no_outliers[,f3])
```

(b) If one or more of the variables should be dropped, redo the scree plot and
the estimation of the factor model with the number of factors suggested by
the scree plot. Report and motivate all the steps you have taken to arrive
at a final factor model. Report the set of included variables, the estimated
uniquenesses and factor loadings of your final factor model.

```{r}
EFA.kfactors <- factanal(no_outliers[,c(8:11,13:17)], factors=3, rotation="varimax")
EFA.kfactors$uniquenesses
EFA.kfactors$loadings
```

3 Factors
1. Info1, Info2, Info 3, Info4: Industry
  - Suppliers of facilities, equipment, components, software and services
  - Sources within your firm or enterprise group
  - Customers or principals
  - Competitor or other enterprise in your industry sector
2. Info6, Info7: Research
  - Universities, universities of applied science or other higher education institutions
  - National or non-profit research institutions
3. Info8, Info9, Info10: Other
  - Conferences, fairs, exhibitions
  - Scientific periodicals and specialty publications
  - Associations and boards

### Question 19
Perform a validity analysis of your factor model by splitting the sample in two ran-
dom partitions and comparing the factor loadings. Report the estimated unique-
nesses and factor loadings of the split samples. What do you conclude?

The factor loadings and uniqueness of variables are very similar between partitions 1 and 2. The variable groupings also remain the same between the partition.
```{r}
Data <- data.frame(no_outliers)
# Identify the number of observations in Data:
N <- dim(Data)[1]
# Create a random index vector:
shuffle.index <- sample(1:N,size=N/2)
# Shuffle the data:
no_outliers.Partition1 <- Data[shuffle.index,]
no_outliers.Partition2 <- Data[-shuffle.index,]
```

```{r}
EFA.kfactors.1 <- factanal(no_outliers.Partition1[,c(8:11,13:17)], factors=3, rotation="varimax")
EFA.kfactors.2 <- factanal(no_outliers.Partition2[,c(8:11,13:17)], factors=3, rotation="varimax")
EFA.kfactors.1
EFA.kfactors.2
```

### Question 20
Report the results of the factor analysis in a table, including all the important
metrics.

```{r}
EFA.3factors <- factanal(no_outliers[,c(8:11,13:17)], factors=3, rotation="varimax", scores="regression")

FactorScores <- data.frame(EFA.3factors$scores)
final.data <- no_outliers[,c(1:7)]
final.data$InfoIndustrySector <- FactorScores$Factor1
final.data$InfoResearchSector <- FactorScores$Factor2
final.data$InfoOther <- FactorScores$Factor3
```


### Question 21
What type of variables are the estimated factors? Choose from nominal scale,
ordinal scale, interval scale or ratio scale and explain your answer.

The Info variables are from the ordinal scale because the interpretation of each option can vary from person to person meaning that the difference between the variables is relative. However, the answer options do have a relative order.

### Question 22
For each type of innovation (product, service, manufacturing, logistics and sup-
port), check whether firms that have introduced such innovations load higher on 
each of the factors than firms that have not. Support your answers by plots and
by statistical tests. What do you conclude?


There is a significant difference between all Info variables and Innovation variables except for InfoResearchSector and LogisticsInnovation.
```{r}

for(info in c("InfoIndustrySector","InfoResearchSector","InfoOther")) {
  for(innovation in c("ProductInnovation","ServiceInnovation","ManufacturingInnovation","LogisticsInnovation")) {
    #hist(final.data[,info], breaks=100, col='red')
    boxplot(final.data[,info] ~ final.data[,innovation])
    cat(info, innovation, "p-value", t.test(final.data[,info] ~ final.data[,innovation])$p.value,"\n")
  }
}
```

### Question 23
Is there a strong correlation between the factor scores and R&D expenditures?
Support your answer by a plot and by a statistical test. What do you conclude?

Plotting the log of log-transformed RDExp against the factor scores does not suggest a clear correlation.
```{r}
plot(log(final.data$RDExp), final.data$InfoIndustrySector)
plot(log(final.data$RDExp), final.data$InfoResearchSector)
plot(log(final.data$RDExp), final.data$InfoOther)
```

The Shapiro-Wilk tests below show that the factors are not normally distributed. So we must use a non-parametric test like Kendall's Tau to test for correlation.
```{r}
# Factors are not normally distributed
shapiro.test(final.data$InfoIndustrySector)
shapiro.test(final.data$InfoResearchSector)
shapiro.test(final.data$InfoOther)
```

The Kendall's Tau test (non-parametric) rejects the null hypothesis of independence for all factors. However, the Kendall's tau values are not large so the relationships are not strong.

```{r}
# tau = 1 is perfect relationship
# The null hypothesis is that there is no association between the variables under study.
cor.test(final.data$RDExp, final.data$InfoIndustrySector,  method="kendall")
cor.test(final.data$RDExp, final.data$InfoResearchSector,  method="kendall")
cor.test(final.data$RDExp, final.data$InfoOther,  method="kendall")
```

